from exp.exp_basic import Exp_Basic
from models import dual,redual,STAR,FusedTimeModel  # ä¿®æ”¹ï¼šå¯¼å…¥æ–°çš„åŒåˆ†æ”¯æ¨¡å‹
from data_provider.data_factory import data_provider
from utils.tools import EarlyStopping, adjust_learning_rate
from utils.metrics import metric
import torch
import torch.nn as nn
from torch import optim
import os
import time
import warnings
import numpy as np
import pandas as pd
from torch.optim import lr_scheduler
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime
import seaborn as sns
from exp.learning_rate import (AdaptiveLossLRScheduler,CombinedLRScheduler,plot_lr_loss_history,)
import math
warnings.filterwarnings('ignore')
import torch
import torch.nn.functional as F


class BarronAdaptiveLoss(nn.Module):
    """Barronè‡ªé€‚åº”æŸå¤±å‡½æ•°æ ¸å¿ƒ"""

    def __init__(self, alpha_init=2.0, scale_init=1.0):
        super().__init__()
        self._alpha = nn.Parameter(torch.tensor(alpha_init, dtype=torch.float32))
        self._scale = nn.Parameter(torch.tensor(scale_init, dtype=torch.float32))

    @property
    def alpha(self):
        return F.softplus(self._alpha) + 0.01

    @property
    def scale(self):
        return F.softplus(self._scale) + 1e-6

    def forward(self, pred, target):
        residual = pred - target
        alpha = self.alpha
        scale = self.scale

        normalized_residual = residual / scale

        if torch.abs(alpha - 2.0) < 1e-4:
            loss = 0.5 * normalized_residual.pow(2)
        else:
            abs_alpha_minus_2 = torch.abs(alpha - 2.0)
            inner_term = (normalized_residual.pow(2) / abs_alpha_minus_2 + 1).pow(alpha / 2) - 1
            loss = (abs_alpha_minus_2 / alpha) * inner_term

        return loss.mean()


class DebugOptimizedCompleteLoss(nn.Module):
    """è°ƒè¯•ç‰ˆè‡ªé€‚åº”æŸå¤±å‡½æ•°"""

    def __init__(self, alpha_init=2.0, scale_init=1.0, recovery_weight=0.3, trend_weight=0.1):
        super().__init__()
        self.barron_loss = BarronAdaptiveLoss(alpha_init, scale_init)
        self.recovery_weight = recovery_weight
        self.trend_weight = trend_weight

        # ğŸ”¥ åŸºäºä½ çš„æ•°æ®åˆ†æçš„ä¼˜åŒ–é˜ˆå€¼
        self.recovery_thresholds = {
            'conservative': 0.000008,  # 89.81% å›å‡æ£€æµ‹ç‡
            'balanced': 0.000049,  # 74.93% å›å‡æ£€æµ‹ç‡
            'strict': 0.000470,  # 49.86% å›å‡æ£€æµ‹ç‡
            'very_strict': 0.001000,  # 39.94% å›å‡æ£€æµ‹ç‡
            'debug': 0.000001  # è¶…ä¿å®ˆç­–ç•¥ï¼Œç”¨äºè°ƒè¯•
        }

        # é»˜è®¤ä½¿ç”¨è°ƒè¯•ç­–ç•¥
        self.current_strategy = 'debug'
        self.recovery_threshold = self.recovery_thresholds[self.current_strategy]

        # ç»Ÿè®¡è®¡æ•°å™¨
        self.total_calls = 0
        self.recovery_detections = 0
        self.debug_counter = 0
        self.strategy_stats = {strategy: 0 for strategy in self.recovery_thresholds.keys()}

        print(f"ğŸš€ åˆå§‹åŒ–è°ƒè¯•ç‰ˆè‡ªé€‚åº”æŸå¤±å‡½æ•°")
        print(f"   ç­–ç•¥: {self.current_strategy}")
        print(f"   é˜ˆå€¼: {self.recovery_threshold:.6f}")
        print(f"   æƒé‡: recovery={recovery_weight}, trend={trend_weight}")

    def set_detection_strategy(self, strategy='debug'):
        """è®¾ç½®æ£€æµ‹ç­–ç•¥"""
        if strategy in self.recovery_thresholds:
            self.current_strategy = strategy
            self.recovery_threshold = self.recovery_thresholds[strategy]
            print(f"ğŸ”„ åˆ‡æ¢æ£€æµ‹ç­–ç•¥: {strategy} (é˜ˆå€¼: {self.recovery_threshold:.6f})")
        else:
            print(f"âŒ æœªçŸ¥ç­–ç•¥: {strategy}")
            print(f"   å¯ç”¨ç­–ç•¥: {list(self.recovery_thresholds.keys())}")

    def set_custom_threshold(self, threshold):
        """è®¾ç½®è‡ªå®šä¹‰é˜ˆå€¼"""
        self.recovery_threshold = threshold
        self.current_strategy = 'custom'
        print(f"ğŸ¯ è®¾ç½®è‡ªå®šä¹‰é˜ˆå€¼: {threshold:.6f}")

    def forward(self, pred, true):
        self.total_calls += 1
        self.debug_counter += 1

        # åŸºç¡€è‡ªé€‚åº”æŸå¤±
        base_loss = self.barron_loss(pred, true)
        total_loss = base_loss

        # ğŸ” è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        debug_info = {
            'pred_shape': pred.shape,
            'true_shape': true.shape,
            'pred_device': pred.device,
            'true_device': true.device,
            'pred_range': (pred.min().item(), pred.max().item()),
            'true_range': (true.min().item(), true.max().item()),
            'has_time_dim': pred.size(1) > 1,
            'detected': False,
            'positive_changes': 0,
            'max_change': 0.0,
            'min_change': 0.0,
            'total_changes': 0
        }

        # å®¹é‡å›å‡æ£€æµ‹å’Œé¢å¤–æƒ©ç½š
        if pred.size(1) > 1:
            # è®¡ç®—çœŸå®å€¼çš„å˜åŒ–ç‡
            true_diff = torch.diff(true, dim=1)
            pred_diff = torch.diff(pred, dim=1)

            # æ›´æ–°è°ƒè¯•ä¿¡æ¯
            debug_info['diff_shape'] = true_diff.shape
            debug_info['diff_device'] = true_diff.device
            debug_info['diff_range'] = (true_diff.min().item(), true_diff.max().item())
            debug_info['total_changes'] = true_diff.numel()

            # ç»Ÿè®¡æ­£å˜åŒ–
            positive_changes = true_diff[true_diff > 0]
            debug_info['positive_changes'] = len(positive_changes)

            if len(positive_changes) > 0:
                debug_info['max_change'] = positive_changes.max().item()
                debug_info['min_change'] = positive_changes.min().item()
                debug_info['avg_change'] = positive_changes.mean().item()

                # æµ‹è¯•ä¸åŒé˜ˆå€¼
                for test_threshold in [0.000001, 0.000008, 0.000049, 0.001]:
                    count = (positive_changes > test_threshold).sum().item()
                    debug_info[f'threshold_{test_threshold:.6f}'] = count

            # ä½¿ç”¨å½“å‰é˜ˆå€¼æ£€æµ‹å›å‡
            recovery_mask = (true_diff > self.recovery_threshold).float()
            debug_info['recovery_mask_sum'] = recovery_mask.sum().item()
            debug_info['threshold_used'] = self.recovery_threshold

            if recovery_mask.sum() > 0:
                debug_info['detected'] = True
                self.recovery_detections += 1
                self.strategy_stats[self.current_strategy] += 1

                # å›å‡æŸå¤±è®¡ç®—
                recovery_error = F.mse_loss(
                    pred[:, 1:] * recovery_mask,
                    true[:, 1:] * recovery_mask,
                    reduction='mean'
                )

                pred_trend = torch.sign(pred_diff)
                true_trend = torch.sign(true_diff)
                trend_consistency = F.mse_loss(
                    pred_trend * recovery_mask,
                    true_trend * recovery_mask,
                    reduction='mean'
                )

                recovery_magnitude_error = F.l1_loss(
                    torch.abs(pred_diff) * recovery_mask,
                    torch.abs(true_diff) * recovery_mask,
                    reduction='mean'
                )

                total_loss = (base_loss +
                              self.recovery_weight * recovery_error +
                              self.trend_weight * trend_consistency +
                              0.1 * recovery_magnitude_error)

                debug_info['recovery_loss'] = recovery_error.item()
                debug_info['trend_loss'] = trend_consistency.item()
                debug_info['magnitude_loss'] = recovery_magnitude_error.item()

        # æ¯10æ¬¡è°ƒç”¨è¾“å‡ºä¸€æ¬¡è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        if self.debug_counter % 10 == 0:
            self._print_debug_info(debug_info)

        # æ¯100æ¬¡è°ƒç”¨è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        if self.total_calls % 100 == 0:
            self._print_statistics()

        return total_loss

    def _print_debug_info(self, debug_info):
        """è¾“å‡ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯"""
        print(f"\nğŸ”¬ è°ƒè¯•ä¿¡æ¯ (è°ƒç”¨: {self.total_calls})")
        print(f"   ğŸ“Š æ•°æ®å½¢çŠ¶: pred={debug_info['pred_shape']}, true={debug_info['true_shape']}")
        print(f"   ğŸ’¾ è®¾å¤‡: pred={debug_info['pred_device']}, true={debug_info['true_device']}")
        print(f"   ğŸ“ˆ æ•°æ®èŒƒå›´: pred=[{debug_info['pred_range'][0]:.6f}, {debug_info['pred_range'][1]:.6f}]")
        print(f"               true=[{debug_info['true_range'][0]:.6f}, {debug_info['true_range'][1]:.6f}]")

        if debug_info['has_time_dim']:
            print(f"   ğŸ” å·®åˆ†åˆ†æ:")
            print(f"      å·®åˆ†å½¢çŠ¶: {debug_info['diff_shape']}")
            print(f"      å·®åˆ†èŒƒå›´: [{debug_info['diff_range'][0]:.6f}, {debug_info['diff_range'][1]:.6f}]")
            print(f"      æ€»å˜åŒ–æ•°: {debug_info['total_changes']}")
            print(f"      æ­£å˜åŒ–æ•°: {debug_info['positive_changes']}")

            if debug_info['positive_changes'] > 0:
                print(f"      æ­£å˜åŒ–èŒƒå›´: [{debug_info['min_change']:.6f}, {debug_info['max_change']:.6f}]")
                print(f"      å¹³å‡æ­£å˜åŒ–: {debug_info.get('avg_change', 0):.6f}")

                print(f"   ğŸ¯ ä¸åŒé˜ˆå€¼æµ‹è¯•:")
                for threshold in [0.000001, 0.000008, 0.000049, 0.001]:
                    count = debug_info.get(f'threshold_{threshold:.6f}', 0)
                    print(f"      é˜ˆå€¼ {threshold:.6f}: {count} ä¸ªæ£€æµ‹ç‚¹")

            print(f"   ğŸš¨ å½“å‰æ£€æµ‹ç»“æœ:")
            print(f"      ä½¿ç”¨é˜ˆå€¼: {debug_info['threshold_used']:.6f}")
            print(f"      æ£€æµ‹åˆ°å›å‡: {'æ˜¯' if debug_info['detected'] else 'å¦'}")
            print(f"      å›å‡æ©ç æ€»å’Œ: {debug_info['recovery_mask_sum']}")

            if debug_info['detected']:
                print(f"      å›å‡æŸå¤±: {debug_info.get('recovery_loss', 0):.6f}")
                print(f"      è¶‹åŠ¿æŸå¤±: {debug_info.get('trend_loss', 0):.6f}")
        else:
            print(f"   âš ï¸  åºåˆ—é•¿åº¦ä¸è¶³ï¼Œæ— æ³•è®¡ç®—å·®åˆ†")

    def _print_statistics(self):
        """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
        alpha_val = self.barron_loss.alpha.item()
        scale_val = self.barron_loss.scale.item()
        detection_rate = self.recovery_detections / self.total_calls * 100

        print(f"\nğŸ“Š ç´¯è®¡ç»Ÿè®¡ (è°ƒç”¨: {self.total_calls})")
        print(f"   è‡ªé€‚åº”å‚æ•°: Î±={alpha_val:.4f}, Ïƒ={scale_val:.4f}")
        print(f"   å›å‡æ£€æµ‹: {self.recovery_detections}æ¬¡ ({detection_rate:.2f}%)")
        print(f"   ç­–ç•¥ç»Ÿè®¡: {dict(self.strategy_stats)}")

    def get_comprehensive_stats(self):
        """è·å–å…¨é¢çš„ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'loss_params': {
                'alpha': self.barron_loss.alpha.item(),
                'scale': self.barron_loss.scale.item(),
                'alpha_raw': self.barron_loss._alpha.item(),
                'scale_raw': self.barron_loss._scale.item()
            },
            'detection_stats': {
                'total_calls': self.total_calls,
                'recovery_detections': self.recovery_detections,
                'detection_rate': self.recovery_detections / max(1, self.total_calls) * 100,
                'current_strategy': self.current_strategy,
                'current_threshold': self.recovery_threshold,
                'strategy_stats': dict(self.strategy_stats)
            },
            'thresholds': dict(self.recovery_thresholds)
        }

    def get_params(self):
        """è·å–å½“å‰å‚æ•°"""
        return {
            'alpha': self.barron_loss.alpha.item(),
            'scale': self.barron_loss.scale.item(),
            'alpha_raw': self.barron_loss._alpha.item(),
            'scale_raw': self.barron_loss._scale.item(),
            'recovery_detections': self.recovery_detections,
            'total_calls': self.total_calls
        }

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.total_calls = 0
        self.recovery_detections = 0
        self.debug_counter = 0
        self.strategy_stats = {strategy: 0 for strategy in self.recovery_thresholds.keys()}


def print_final_recovery_report(criterion):
    """æ‰“å°æœ€ç»ˆçš„å›å‡æ£€æµ‹æŠ¥å‘Š"""
    if isinstance(criterion, OptimizedCompleteLoss):
        stats = criterion.get_comprehensive_stats()

        print(f"\n" + "=" * 60)
        print(f"ğŸ¯ æœ€ç»ˆå›å‡æ£€æµ‹æŠ¥å‘Š")
        print(f"=" * 60)

        # æŸå¤±å‚æ•°å˜åŒ–
        loss_params = stats['loss_params']
        print(f"ğŸ“Š è‡ªé€‚åº”æŸå¤±å‚æ•°:")
        print(f"   æœ€ç»ˆ Î±: {loss_params['alpha']:.4f}")
        print(f"   æœ€ç»ˆ Ïƒ: {loss_params['scale']:.4f}")

        # æ£€æµ‹ç»Ÿè®¡
        detection_stats = stats['detection_stats']
        print(f"\nğŸ” å›å‡æ£€æµ‹ç»Ÿè®¡:")
        print(f"   æ€»è°ƒç”¨æ¬¡æ•°: {detection_stats['total_calls']}")
        print(f"   å›å‡æ£€æµ‹æ¬¡æ•°: {detection_stats['recovery_detections']}")
        print(f"   æ£€æµ‹ç‡: {detection_stats['detection_rate']:.2f}%")
        print(f"   æœ€ç»ˆç­–ç•¥: {detection_stats['current_strategy']}")
        print(f"   æœ€ç»ˆé˜ˆå€¼: {detection_stats['current_threshold']:.6f}")

        # ç­–ç•¥ä½¿ç”¨ç»Ÿè®¡
        print(f"\nğŸ“ˆ ç­–ç•¥ä½¿ç”¨ç»Ÿè®¡:")
        for strategy, count in detection_stats['strategy_stats'].items():
            percentage = count / max(1, detection_stats['recovery_detections']) * 100
            print(f"   {strategy:12s}: {count:6d} æ¬¡ ({percentage:5.1f}%)")

        print(f"=" * 60)


def generate_timestamp():
    """ç”Ÿæˆæ—¶é—´æˆ³å­—ç¬¦ä¸²"""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def generate_detailed_timestamp():
    """ç”Ÿæˆè¯¦ç»†çš„æ—¶é—´æˆ³å­—ç¬¦ä¸²"""
    return datetime.now().strftime("%Y-%m-%d_%H-%M-%S")


class Exp_Fused_Forecast(Exp_Basic):
    def __init__(self, args):
        super(Exp_Fused_Forecast, self).__init__(args)
        self.experiment_timestamp = generate_timestamp()
        self.detailed_timestamp = generate_detailed_timestamp()
        print(f"å®éªŒæ—¶é—´æˆ³: {self.experiment_timestamp}")
        print(f"è¯¦ç»†æ—¶é—´æˆ³: {self.detailed_timestamp}")

    def _build_model(self):
        model = self.model_dict[self.args.model].Model(self.args).float()
        if self.args.use_multi_gpu and self.args.use_gpu:
            model = nn.DataParallel(model, device_ids=self.args.device_ids)
        return model

    def _get_data(self, flag):
        data_set, data_loader = data_provider(self.args, flag)
        return data_set, data_loader

    def _select_criterion(self):
        """è°ƒè¯•ç‰ˆæŸå¤±å‡½æ•°é€‰æ‹©"""
        if getattr(self.args, 'use_adaptive_loss', False):
            print("ğŸ” ä½¿ç”¨è°ƒè¯•ç‰ˆè‡ªé€‚åº”æŸå¤±å‡½æ•°")

            strategy = getattr(self.args, 'recovery_strategy', 'debug')  # é»˜è®¤ä½¿ç”¨debugç­–ç•¥
            custom_threshold = getattr(self.args, 'custom_recovery_threshold', None)

            debug_loss = DebugOptimizedCompleteLoss(
                alpha_init=2.0,
                scale_init=1.0,
                recovery_weight=getattr(self.args, 'recovery_weight', 0.3),
                trend_weight=getattr(self.args, 'trend_weight', 0.1)
            )

            # è®¾ç½®æ£€æµ‹ç­–ç•¥
            if custom_threshold is not None:
                debug_loss.set_custom_threshold(custom_threshold)
            else:
                debug_loss.set_detection_strategy(strategy)

            if hasattr(self, 'device'):
                debug_loss = debug_loss.to(self.device)
            elif torch.cuda.is_available():
                debug_loss = debug_loss.cuda()

            return debug_loss
        else:
            print("ğŸ“Š ä½¿ç”¨æ ‡å‡† MSE æŸå¤±å‡½æ•°")
            return nn.MSELoss()

    def _select_optimizer(self):
        """ä¿®æ”¹ä¼˜åŒ–å™¨é€‰æ‹©ï¼ŒåŒ…å«æŸå¤±å‡½æ•°å‚æ•°"""
        # å…ˆè·å–æŸå¤±å‡½æ•°
        criterion = self._select_criterion()

        # æ”¶é›†æ‰€æœ‰éœ€è¦ä¼˜åŒ–çš„å‚æ•°
        model_params = list(self.model.parameters())

        if isinstance(criterion, DebugOptimizedCompleteLoss):
            # åŒ…å«æŸå¤±å‡½æ•°çš„å‚æ•°
            loss_params = list(criterion.parameters())
            all_params = model_params + loss_params
            print(f"âœ… ä¼˜åŒ–å™¨åŒ…å«: æ¨¡å‹å‚æ•° {len(model_params)} + æŸå¤±å‚æ•° {len(loss_params)} = {len(all_params)}")
        else:
            all_params = model_params
            print(f"ğŸ“Š ä¼˜åŒ–å™¨åŒ…å«: æ¨¡å‹å‚æ•° {len(model_params)}")

        model_optim = optim.Adam(all_params, lr=self.args.learning_rate)

        # ä¿å­˜criterionä»¥ä¾›åç»­ä½¿ç”¨
        self.criterion = criterion

        return model_optim

    def vali(self, vali_data, vali_loader, criterion):
        total_loss = []
        preds = []
        trues = []

        self.model.eval()
        with torch.no_grad():
            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):
                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)
                batch_x_mark = batch_x_mark.float().to(self.device)
                batch_y_mark = batch_y_mark.float().to(self.device)

                # æ¨¡å‹å‰å‘
                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)

                f_dim = -1 if self.args.features == 'MS' else 0
                outputs = outputs[:, -self.args.pred_len:, f_dim:]
                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]

                # ç›´æ¥è°ƒç”¨æŸå¤±å‡½æ•°
                loss = criterion(outputs, batch_y)
                total_loss.append(loss.item())

                # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼ç”¨äºè®¡ç®—RÂ²
                preds.append(outputs.detach().cpu().numpy())
                trues.append(batch_y.detach().cpu().numpy())

        total_loss = np.average(total_loss)

        # è®¡ç®—éªŒè¯é›†ä¸Šçš„RÂ²
        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)
        _, _, _, _, _ = metric(preds, trues)
        from sklearn.metrics import r2_score
        r2 = r2_score(trues.flatten(), preds.flatten())

        self.model.train()
        return total_loss, r2

    def train(self, setting):
        """ä¿®æ”¹åçš„è®­ç»ƒæ–¹æ³•ï¼Œå®Œæ•´é›†æˆè‡ªé€‚åº”æŸå¤±å‡½æ•°"""
        train_data, train_loader = self._get_data(flag='train')
        vali_data, vali_loader = self._get_data(flag='val')
        test_data, test_loader = self._get_data(flag='test')

        timestamped_setting = f"{setting}_{self.experiment_timestamp}"
        path = os.path.join(self.args.checkpoints, timestamped_setting)
        if not os.path.exists(path):
            os.makedirs(path)

        time_now = time.time()
        train_steps = len(train_loader)

        early_stopping = EarlyStopping(patience=5, verbose=True)
        max_epochs = 20

        # é‡è¦ï¼šå…ˆé€‰æ‹©ä¼˜åŒ–å™¨ï¼ˆå†…éƒ¨ä¼šè°ƒç”¨_select_criterionï¼‰
        model_optim = self._select_optimizer()
        criterion = self.criterion  # ä½¿ç”¨ä¿å­˜çš„criterion

        # è¾“å‡ºæŸå¤±å‡½æ•°ä¿¡æ¯
        if isinstance(criterion, DebugOptimizedCompleteLoss):
            print("âœ… å¯ç”¨å®Œæ•´è‡ªé€‚åº”æŸå¤±å‡½æ•°ï¼ŒåŒ…å«å®¹é‡å›å‡æ£€æµ‹")
            print(f"ğŸ“‹ åˆå§‹å‚æ•°: Î±={criterion.get_params()['alpha']:.3f}, Ïƒ={criterion.get_params()['scale']:.3f}")
        else:
            print("ğŸ“Œ ä½¿ç”¨æ ‡å‡†MSEæŸå¤±å‡½æ•°")

        # ===== å­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½® =====
        if hasattr(self.args, 'lradj') and self.args.lradj == 'adaptive':
            scheduler = AdaptiveLossLRScheduler(
                optimizer=model_optim,
                patience=1,
                factor=0.5,
                min_lr=1e-7,
                verbose=True,
                threshold=1e-4,
                cooldown=2
            )
            use_adaptive = True
        elif hasattr(self.args, 'lradj') and self.args.lradj == 'combined':
            scheduler = CombinedLRScheduler(
                optimizer=model_optim,
                T_max=max_epochs,
                eta_min=1e-6,
                adaptive_patience=1,
                adaptive_factor=0.3,
                min_lr=1e-8,
                verbose=True
            )
            use_adaptive = True
        elif hasattr(self.args, 'lradj') and self.args.lradj == 'plateau':
            from torch.optim.lr_scheduler import ReduceLROnPlateau
            scheduler = ReduceLROnPlateau(
                optimizer=model_optim,
                mode='min',
                factor=0.5,
                patience=1,
                verbose=True,
                threshold=1e-4,
                min_lr=1e-7
            )
            use_adaptive = True
        else:
            if hasattr(self.args, 'lradj') and self.args.lradj == 'TST':
                scheduler = lr_scheduler.OneCycleLR(
                    optimizer=model_optim,
                    steps_per_epoch=train_steps,
                    pct_start=self.args.pct_start,
                    epochs=max_epochs,
                    max_lr=self.args.learning_rate
                )
            else:
                scheduler = None
            use_adaptive = False

        # è®°å½•è®­ç»ƒå†å²
        train_history = {
            'train_loss': [],
            'vali_loss': [],
            'vali_r2': [],
            'test_loss': [],
            'test_r2': [],
            'learning_rate': []
        }

        # å¦‚æœä½¿ç”¨è‡ªé€‚åº”æŸå¤±ï¼Œè®°å½•æŸå¤±å‚æ•°å˜åŒ–
        if isinstance(criterion, DebugOptimizedCompleteLoss):
            train_history['loss_alpha'] = []
            train_history['loss_scale'] = []
            train_history['recovery_detections'] = []

        # åˆ›å»ºæ€»çš„epochè¿›åº¦æ¡
        epoch_pbar = tqdm(range(max_epochs), desc="Training Epochs", unit="epoch")

        for epoch in epoch_pbar:
            iter_count = 0
            train_loss = []
            self.model.train()
            epoch_time = time.time()

            # é‡ç½®æŸå¤±å‡½æ•°ç»Ÿè®¡ï¼ˆæ¯ä¸ªepochå¼€å§‹ï¼‰
            if isinstance(criterion, DebugOptimizedCompleteLoss):
                criterion.reset_stats()

            # ä¸ºæ¯ä¸ªepochçš„batchåˆ›å»ºè¿›åº¦æ¡
            batch_pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{max_epochs}",
                              leave=False, unit="batch")

            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(batch_pbar):
                iter_count += 1
                model_optim.zero_grad()

                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)
                batch_x_mark = batch_x_mark.float().to(self.device)
                batch_y_mark = batch_y_mark.float().to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)

                f_dim = -1 if self.args.features == 'MS' else 0
                outputs = outputs[:, -self.args.pred_len:, f_dim:]
                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]

                # è®¡ç®—æŸå¤±
                loss = criterion(outputs, batch_y)
                train_loss.append(loss.item())

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                current_lr = model_optim.param_groups[0]['lr']

                if isinstance(criterion, DebugOptimizedCompleteLoss):
                    params = criterion.get_params()
                    batch_pbar.set_postfix({
                        'Loss': f"{loss.item():.6f}",
                        'LR': f"{current_lr:.2e}",
                        'Î±': f"{params['alpha']:.3f}",
                        'Ïƒ': f"{params['scale']:.3f}"
                    })
                else:
                    batch_pbar.set_postfix({
                        'Loss': f"{loss.item():.6f}",
                        'LR': f"{current_lr:.2e}"
                    })

                if (i + 1) % 100 == 0:
                    speed = (time.time() - time_now) / iter_count
                    left_time = speed * ((max_epochs - epoch) * train_steps - i)
                    tqdm.write(f"\titers: {i + 1}, epoch: {epoch + 1} | loss: {loss.item():.7f} | lr: {current_lr:.2e}")
                    tqdm.write(f'\tspeed: {speed:.4f}s/iter; left time: {left_time:.4f}s')

                    # è¾“å‡ºè‡ªé€‚åº”æŸå¤±å‚æ•°
                    if isinstance(criterion, DebugOptimizedCompleteLoss):
                        params = criterion.get_params()
                        tqdm.write(
                            f'\tè‡ªé€‚åº”æŸå¤±: Î±={params["alpha"]:.4f}, Ïƒ={params["scale"]:.4f}, å›å‡æ£€æµ‹={params["recovery_detections"]}æ¬¡')

                    iter_count = 0
                    time_now = time.time()

                loss.backward()
                model_optim.step()

                # å¯¹äºOneCycleLRï¼Œåœ¨æ¯ä¸ªbatchåè°ƒç”¨
                if hasattr(self.args, 'lradj') and self.args.lradj == 'TST' and scheduler is not None:
                    scheduler.step()

            batch_pbar.close()

            epoch_cost_time = time.time() - epoch_time
            train_loss = np.average(train_loss)
            vali_loss, vali_r2 = self.vali(vali_data, vali_loader, criterion)
            test_loss, test_r2 = self.vali(test_data, test_loader, criterion)

            # è®°å½•å½“å‰å­¦ä¹ ç‡
            current_lr = model_optim.param_groups[0]['lr']

            # è®°å½•å†å²
            train_history['train_loss'].append(train_loss)
            train_history['vali_loss'].append(vali_loss)
            train_history['vali_r2'].append(vali_r2)
            train_history['test_loss'].append(test_loss)
            train_history['test_r2'].append(test_r2)
            train_history['learning_rate'].append(current_lr)

            # å¦‚æœä½¿ç”¨è‡ªé€‚åº”æŸå¤±ï¼Œè®°å½•æŸå¤±å‚æ•°
            if isinstance(criterion, DebugOptimizedCompleteLoss):
                params = criterion.get_params()
                train_history['loss_alpha'].append(params['alpha'])
                train_history['loss_scale'].append(params['scale'])
                train_history['recovery_detections'].append(params['recovery_detections'])

            # ===== å­¦ä¹ ç‡è°ƒåº¦ =====
            if use_adaptive:
                if hasattr(self.args, 'lradj') and self.args.lradj in ['adaptive', 'combined']:
                    scheduler.step(vali_loss, epoch)
                elif hasattr(self.args, 'lradj') and self.args.lradj == 'plateau':
                    scheduler.step(vali_loss)
            else:
                if hasattr(self.args, 'lradj') and self.args.lradj != 'TST':
                    adjust_learning_rate(model_optim, epoch + 1, self.args)

            # æ£€æŸ¥å­¦ä¹ ç‡å˜åŒ–
            new_lr = model_optim.param_groups[0]['lr']
            if abs(new_lr - current_lr) > 1e-10:
                tqdm.write(f"Learning rate changed from {current_lr:.2e} to {new_lr:.2e}")

            # è¾“å‡ºepochç»“æœ
            tqdm.write(f"Epoch: {epoch + 1} cost time: {epoch_cost_time:.2f}s")
            tqdm.write(
                f"Epoch: {epoch + 1}, Steps: {train_steps} | Train Loss: {train_loss:.7f} Vali Loss: {vali_loss:.7f} Test Loss: {test_loss:.7f}")
            tqdm.write(f"Vali RÂ²: {vali_r2:.4f} Test RÂ²: {test_r2:.4f} | LR: {new_lr:.2e}")

            # è¾“å‡ºè‡ªé€‚åº”æŸå¤±å‚æ•°å˜åŒ–
            if isinstance(criterion, DebugOptimizedCompleteLoss):
                params = criterion.get_params()
                tqdm.write(
                    f"è‡ªé€‚åº”æŸå¤±å‚æ•°: Î±={params['alpha']:.4f}, Ïƒ={params['scale']:.4f}, æœ¬epochå›å‡æ£€æµ‹={params['recovery_detections']}æ¬¡")

            # æ›´æ–°epochè¿›åº¦æ¡
            if isinstance(criterion, DebugOptimizedCompleteLoss):
                params = criterion.get_params()
                epoch_pbar.set_postfix({
                    'Train_Loss': f"{train_loss:.6f}",
                    'Vali_R2': f"{vali_r2:.4f}",
                    'Test_R2': f"{test_r2:.4f}",
                    'Î±': f"{params['alpha']:.3f}",
                    'Ïƒ': f"{params['scale']:.3f}"
                })
            else:
                epoch_pbar.set_postfix({
                    'Train_Loss': f"{train_loss:.6f}",
                    'Vali_R2': f"{vali_r2:.4f}",
                    'Test_R2': f"{test_r2:.4f}",
                    'LR': f"{new_lr:.2e}"
                })

            # æ—©åœæ£€æŸ¥
            early_stopping(vali_loss, self.model, path)
            if early_stopping.early_stop:
                tqdm.write("Early stopping triggered!")
                tqdm.write(f"Training stopped at epoch {epoch + 1}")
                break

        epoch_pbar.close()

        # ä¿å­˜è®­ç»ƒå†å²
        history_file = os.path.join(path, f'training_history_{self.experiment_timestamp}.json')
        train_history_serializable = {}
        for key, value in train_history.items():
            if isinstance(value[0], (int, float)):
                train_history_serializable[key] = [float(x) for x in value]
            else:
                train_history_serializable[key] = value

        import json
        with open(history_file, 'w') as f:
            json.dump(train_history_serializable, f, indent=2)

        # å¦‚æœä½¿ç”¨è‡ªé€‚åº”è°ƒåº¦å™¨ï¼Œä¿å­˜å¯è§†åŒ–
        if use_adaptive and isinstance(scheduler, (AdaptiveLossLRScheduler, CombinedLRScheduler)):
            try:
                scheduler_state_file = os.path.join(path, f'scheduler_state_{self.experiment_timestamp}.json')
                if hasattr(scheduler, 'state_dict'):
                    with open(scheduler_state_file, 'w') as f:
                        json.dump(scheduler.state_dict(), f, indent=2)

                lr_plot_path = os.path.join(path, f'lr_loss_history_{self.experiment_timestamp}.png')
                plot_lr_loss_history(scheduler, lr_plot_path)
            except Exception as e:
                tqdm.write(f"Warning: Could not save scheduler visualization: {e}")

        best_model_path = path + '/' + 'checkpoint.pth'
        self.model.load_state_dict(torch.load(best_model_path))

        # æœ€ç»ˆè®­ç»ƒå®Œæˆåè®¡ç®—è®­ç»ƒé›†RÂ²
        tqdm.write("Computing final training metrics...")
        final_train_loss, final_train_r2 = self.train_step_metrics(train_loader, criterion)
        tqdm.write(f"Final Training RÂ²: {final_train_r2:.6f}")

        # è¾“å‡ºå­¦ä¹ ç‡è°ƒåº¦æ€»ç»“
        if use_adaptive:
            final_lr = model_optim.param_groups[0]['lr']
            initial_lr = self.args.learning_rate
            tqdm.write(f"Learning rate summary: Initial: {initial_lr:.2e}, Final: {final_lr:.2e}")
            if hasattr(scheduler, 'lr_history') and scheduler.lr_history:
                min_lr = min(scheduler.lr_history)
                tqdm.write(f"Minimum learning rate reached: {min_lr:.2e}")

        # è¾“å‡ºè‡ªé€‚åº”æŸå¤±æ€»ç»“
        if isinstance(criterion, DebugOptimizedCompleteLoss):
            print_final_recovery_report(criterion)

    def train_step_metrics(self, train_loader, criterion):
        """è®¡ç®—è®­ç»ƒé›†ä¸Šçš„RÂ²æŒ‡æ ‡"""
        train_preds = []
        train_trues = []
        train_loss = []

        self.model.eval()
        with torch.no_grad():
            metric_pbar = tqdm(enumerate(train_loader), desc="Computing train metrics",
                               total=min(100, len(train_loader)), leave=False)

            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in metric_pbar:
                if i >= 100:
                    break

                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)
                batch_x_mark = batch_x_mark.float().to(self.device)
                batch_y_mark = batch_y_mark.float().to(self.device)

                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)

                f_dim = -1 if self.args.features == 'MS' else 0
                outputs = outputs[:, -self.args.pred_len:, f_dim:]
                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]

                loss = criterion(outputs, batch_y)
                train_loss.append(loss.item())

                train_preds.append(outputs.detach().cpu().numpy())
                train_trues.append(batch_y.detach().cpu().numpy())

            metric_pbar.close()

        if train_preds:
            train_preds = np.concatenate(train_preds, axis=0)
            train_trues = np.concatenate(train_trues, axis=0)
            _, _, _, _, _ = metric(train_preds, train_trues)
            from sklearn.metrics import r2_score
            train_r2 = r2_score(train_trues.flatten(), train_preds.flatten())
        else:
            train_r2 = 0.0

        self.model.train()
        return np.average(train_loss), train_r2

    def test(self, setting, test=0):
        test_data, test_loader = self._get_data(flag='test')

        # ä¿®æ”¹ï¼šæ·»åŠ æ—¶é—´æˆ³åˆ°è®¾ç½®
        timestamped_setting = f"{setting}_{self.experiment_timestamp}"

        if test:
            print('loading model')
            self.model.load_state_dict(
                torch.load(os.path.join('./checkpoints/' + timestamped_setting, 'checkpoint.pth')))

        preds = []
        trues = []

        # ä¿®æ”¹ï¼šåˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æµ‹è¯•ç»“æœæ–‡ä»¶å¤¹
        folder_path = './test_results/' + timestamped_setting + '/'
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

        self.model.eval()
        with torch.no_grad():
            test_pbar = tqdm(test_loader, desc="Testing", unit="batch")

            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_pbar):
                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)
                batch_x_mark = batch_x_mark.float().to(self.device)
                batch_y_mark = batch_y_mark.float().to(self.device)

                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)

                f_dim = -1 if self.args.features == 'MS' else 0
                outputs = outputs[:, -self.args.pred_len:, f_dim:]
                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]

                pred = outputs.detach().cpu().numpy()
                true = batch_y.detach().cpu().numpy()

                preds.append(pred)
                trues.append(true)

            test_pbar.close()

        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)

        mae, mse, rmse, mape, mspe = metric(preds, trues)
        from sklearn.metrics import r2_score
        r2 = r2_score(trues.flatten(), preds.flatten())
        print(f'mse:{mse:.6f}, mae:{mae:.6f}')
        print(f'rmse:{rmse:.6f}, mape:{mape:.6f}, mspe:{mspe:.6f}')
        print(f'RÂ²:{r2:.6f}')

        # ä¿®æ”¹ï¼šä¿å­˜ç»“æœåˆ°å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å¤¹
        folder_path = './results/' + timestamped_setting + '/'
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

        # === åå½’ä¸€åŒ–é€»è¾‘ä¿æŒä¸å˜ ===
        print("å¼€å§‹åå½’ä¸€åŒ–å¤„ç†...")
        print(f"åŸå§‹predså½¢çŠ¶: {preds.shape}")
        print(f"åŸå§‹trueså½¢çŠ¶: {trues.shape}")

        # è·å–åŸå§‹æµ‹è¯•æ•°æ®
        raw_df = test_data.raw_test_df
        print("åŸå§‹æ•°æ®åˆ—å:", raw_df.columns.tolist())

        # è®¡ç®—é¢„æµ‹æ•°æ®çš„æ•°é‡
        num_preds = len(preds.flatten())
        print(f"é¢„æµ‹æ•°æ®ç‚¹æ•°é‡: {num_preds}")

        # æ–¹æ³•1ï¼šå¦‚æœfeatures=='S'ï¼ˆå•å˜é‡ï¼‰ï¼Œç›´æ¥åå½’ä¸€åŒ–
        if self.args.features == 'S':
            print("ä½¿ç”¨å•å˜é‡åå½’ä¸€åŒ–æ–¹æ³•")
            preds_unscaled = test_data.inverse_transform(preds.reshape(-1, 1)).flatten()
            trues_unscaled = test_data.inverse_transform(trues.reshape(-1, 1)).flatten()
        else:
            print("ä½¿ç”¨å¤šå˜é‡åå½’ä¸€åŒ–æ–¹æ³•")
            feature_dim = test_data.data_x.shape[-1]
            print(f"ç‰¹å¾ç»´åº¦: {feature_dim}")

            if hasattr(test_data, 'scaler'):
                test_mean = np.mean(test_data.data_x, axis=0)

                preds_full = np.tile(test_mean, (num_preds, 1))
                preds_full[:, -1] = preds.flatten()

                trues_full = np.tile(test_mean, (num_preds, 1))
                trues_full[:, -1] = trues.flatten()

                preds_unscaled = test_data.inverse_transform(preds_full)[:, -1]
                trues_unscaled = test_data.inverse_transform(trues_full)[:, -1]
            else:
                print("è­¦å‘Šï¼šæ— æ³•æ‰¾åˆ°scalerï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                preds_unscaled = preds.flatten()
                trues_unscaled = trues.flatten()

        print(f"åå½’ä¸€åŒ–åé¢„æµ‹å€¼èŒƒå›´: [{preds_unscaled.min():.6f}, {preds_unscaled.max():.6f}]")
        print(f"åå½’ä¸€åŒ–åçœŸå®å€¼èŒƒå›´: [{trues_unscaled.min():.6f}, {trues_unscaled.max():.6f}]")

        # è·å–å¯¹åº”çš„cycleæ•°æ®
        start_idx = self.args.seq_len
        end_idx = start_idx + num_preds

        print(f"ä»åŸå§‹æ•°æ®è·å–cycleï¼Œç´¢å¼•èŒƒå›´: {start_idx} åˆ° {end_idx}")
        print(f"åŸå§‹æ•°æ®é•¿åº¦: {len(raw_df)}")

        # æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰Cycleåˆ—
        cycle_col = None
        date_col = None

        possible_cycle_names = ['Cycle', 'cycle', 'CYCLE', 'cycle_number', 'Cycle_Number']
        for col in possible_cycle_names:
            if col in raw_df.columns:
                cycle_col = col
                break

        possible_date_names = ['date', 'Date', 'DATE', 'time', 'Time', 'timestamp']
        for col in possible_date_names:
            if col in raw_df.columns:
                date_col = col
                break

        print(f"æ‰¾åˆ°çš„Cycleåˆ—: {cycle_col}")
        print(f"æ‰¾åˆ°çš„Dateåˆ—: {date_col}")

        if end_idx <= len(raw_df):
            if cycle_col is not None:
                cycle_data = raw_df[cycle_col].values[start_idx:end_idx]
                print("ä½¿ç”¨æ•°æ®é›†ä¸­çš„Cycleåˆ—")
            else:
                cycle_data = np.arange(start_idx + 1, end_idx + 1)
                print("æ•°æ®é›†ä¸­æ²¡æœ‰Cycleåˆ—ï¼Œä½¿ç”¨ç”Ÿæˆçš„åºå·")

            if date_col is not None:
                date_data = raw_df[date_col].values[start_idx:end_idx]
            else:
                date_data = None

            true_targets = raw_df[self.args.target].values[start_idx:end_idx]
        else:
            print("è­¦å‘Šï¼šç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œè°ƒæ•´ç´¢å¼•")
            if cycle_col is not None:
                cycle_data = raw_df[cycle_col].values[-num_preds:]
            else:
                cycle_data = np.arange(len(raw_df) - num_preds + 1, len(raw_df) + 1)

            if date_col is not None:
                date_data = raw_df[date_col].values[-num_preds:]
            else:
                date_data = None

            true_targets = raw_df[self.args.target].values[-num_preds:]

        print(f"è·å–çš„cycleæ•°æ®é•¿åº¦: {len(cycle_data)}")
        print(f"cycleæ•°æ®ç±»å‹: {type(cycle_data[0])}")
        print(f"cycleæ•°æ®å‰5ä¸ª: {cycle_data[:5]}")
        print(f"è·å–çš„çœŸå®targeté•¿åº¦: {len(true_targets)}")
        print(f"çœŸå®targetèŒƒå›´: [{true_targets.min():.6f}, {true_targets.max():.6f}]")

        min_length = min(len(cycle_data), len(true_targets), len(preds_unscaled))
        print(f"æœ€ç»ˆä½¿ç”¨çš„æ•°æ®é•¿åº¦: {min_length}")

        # ä¿®æ”¹ï¼šåˆ›å»ºç»“æœDataFrameæ—¶æ·»åŠ å®éªŒä¿¡æ¯
        results_df = pd.DataFrame({
            'Cycle': cycle_data[:min_length],
            'True_Target': true_targets[:min_length],
            'Predicted_Target': preds_unscaled[:min_length]
        })

        # æ·»åŠ å®éªŒå…ƒä¿¡æ¯
        experiment_info = pd.DataFrame({
            'Experiment_Timestamp': [self.detailed_timestamp] * min_length,
            'Model_Name': [self.args.model] * min_length,
            'Dataset': [self.args.data] * min_length,
            'Data_Path': [self.args.data_path] * min_length,
            'MSE': [mse] * min_length,
            'MAE': [mae] * min_length,
            'RMSE': [rmse] * min_length,
            'R2': [r2] * min_length,
            'Final_d_model': [self.args.d_model] * min_length,
            'Final_learning_rate': [self.args.learning_rate] * min_length,
            'Final_dropout': [self.args.dropout] * min_length,
        })

        # åˆå¹¶å®éªŒä¿¡æ¯å’Œç»“æœ
        detailed_results_df = pd.concat([experiment_info, results_df], axis=1)

        # ä¿å­˜è¯¦ç»†ç»“æœCSVæ–‡ä»¶
        results_csv_path = os.path.join(folder_path, f'forecast_results_{self.experiment_timestamp}.csv')
        detailed_results_df.to_csv(results_csv_path, index=False)
        print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³ {results_csv_path}")

        # åŒæ—¶ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¿æŒåŸæœ‰æ ¼å¼å…¼å®¹æ€§ï¼‰
        simple_results_csv_path = os.path.join(folder_path, 'forecast_results.csv')
        results_df.to_csv(simple_results_csv_path, index=False)
        print(f"ç®€åŒ–ç»“æœå·²ä¿å­˜è‡³ {simple_results_csv_path}")

        print("å‰5è¡Œç»“æœé¢„è§ˆ:")
        print(results_df.head())

        print("å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        # è®¾ç½®matplotlibçš„ç§‘æŠ€è®ºæ–‡é£æ ¼
        plt.style.use('seaborn-v0_8-whitegrid')
        sns.set_palette("husl")

        # åˆ›å»ºå›¾å½¢å’Œè½´
        fig, ax = plt.subplots(figsize=(12, 8), dpi=300)

        # ç¡®å®šxè½´æ•°æ®ï¼šä¼˜å…ˆä½¿ç”¨Cycleï¼Œå¦‚æœç”¨æˆ·æ˜ç¡®éœ€è¦dateåˆ™ä½¿ç”¨date
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨Cycleä½œä¸ºxè½´ï¼Œå› ä¸ºè¿™æ˜¯ç”µæ± ç ”ç©¶çš„æ ‡å‡†åšæ³•
        x_data = cycle_data[:min_length]
        x_label = 'Cycle'

        print(f"ä½¿ç”¨xè½´æ•°æ®: {x_label}")
        print(f"xè½´æ•°æ®èŒƒå›´: {x_data.min()} åˆ° {x_data.max()}")

        # ç»˜åˆ¶çœŸå®å€¼ï¼ˆè“çº¿ï¼‰
        ax.plot(x_data, true_targets[:min_length],
                color='#2E86AB', linewidth=2.5, alpha=0.8,
                label='True SoH', marker='o', markersize=3, markevery=max(1, min_length // 50))

        # ç»˜åˆ¶é¢„æµ‹å€¼ï¼ˆçº¢çº¿ï¼‰
        ax.plot(x_data, preds_unscaled[:min_length],
                color='#F24236', linewidth=2.5, alpha=0.8,
                label='Predicted SoH', marker='s', markersize=3, markevery=max(1, min_length // 50))

        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title('Battery SoH Prediction - Dual Branch Model', fontsize=20, fontweight='bold', pad=20)
        ax.set_ylabel('State of Health (SoH)', fontsize=14, fontweight='bold')
        ax.set_xlabel(x_label, fontsize=14, fontweight='bold')

        # è®¾ç½®ç½‘æ ¼
        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
        ax.set_axisbelow(True)

        # æ·»åŠ å›¾ä¾‹ï¼ˆå³ä¸Šè§’ï¼‰
        legend = ax.legend(loc='upper right', fontsize=12, frameon=True,
                           fancybox=True, shadow=True, framealpha=0.9,
                           bbox_to_anchor=(0.98, 0.98))
        legend.get_frame().set_facecolor('white')
        legend.get_frame().set_edgecolor('gray')
        legend.get_frame().set_linewidth(0.8)

        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ–‡æœ¬æ¡†ï¼ˆå³ä¸Šè§’ï¼Œå›¾ä¾‹ä¸‹æ–¹ï¼‰
        textstr = f'MAE: {mae:.4f}\nMSE: {mse:.4f}\nRMSE: {rmse:.4f}\nRÂ²: {r2:.4f}'
        props = dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8, edgecolor='gray')
        ax.text(0.98, 0.75, textstr, transform=ax.transAxes, fontsize=10,
                verticalalignment='top', horizontalalignment='right',
                bbox=props, family='monospace')

        # è®¾ç½®è½´çš„èŒƒå›´å’Œåˆ»åº¦
        y_min, y_max = min(np.min(true_targets[:min_length]), np.min(preds_unscaled[:min_length])), \
            max(np.max(true_targets[:min_length]), np.max(preds_unscaled[:min_length]))
        y_range = y_max - y_min
        ax.set_ylim(y_min - 0.05 * y_range, y_max + 0.05 * y_range)

        # ç¾åŒ–åˆ»åº¦
        ax.tick_params(axis='both', which='major', labelsize=11, width=1.2, length=6)
        ax.tick_params(axis='both', which='minor', width=0.8, length=3)

        # è®¾ç½®è¾¹æ¡†
        for spine in ax.spines.values():
            spine.set_linewidth(1.2)
            spine.set_color('gray')

        # æ·»åŠ é˜´å½±åŒºåŸŸæ˜¾ç¤ºé¢„æµ‹è¯¯å·®
        error = np.abs(true_targets[:min_length] - preds_unscaled[:min_length])
        ax.fill_between(x_data,
                        preds_unscaled[:min_length] - error / 2,
                        preds_unscaled[:min_length] + error / 2,
                        alpha=0.2, color='red', label='Prediction Error Band')

        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()

        # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
        plot_path = os.path.join(folder_path, 'dual_branch_battery_soh_prediction.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight',
                    facecolor='white', edgecolor='none')

        # åŒæ—¶ä¿å­˜PDFæ ¼å¼ï¼ˆé€‚åˆè®ºæ–‡ä½¿ç”¨ï¼‰
        pdf_path = os.path.join(folder_path, 'dual_branch_battery_soh_prediction.pdf')
        plt.savefig(pdf_path, bbox_inches='tight',
                    facecolor='white', edgecolor='none')

        print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜:")
        print(f"PNGæ ¼å¼: {plot_path}")
        print(f"PDFæ ¼å¼: {pdf_path}")

        # æ˜¾ç¤ºå›¾è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœåœ¨jupyter notebookä¸­è¿è¡Œï¼‰
        plt.show()
        plt.close()

        # =================== å¯è§†åŒ–åŠŸèƒ½ç»“æŸ ===================

        # ä¿å­˜æŒ‡æ ‡
        np.save(folder_path + f'metrics_{self.experiment_timestamp}.npy', np.array([mae, mse, rmse, mape, mspe, r2]))
        np.save(folder_path + f'pred_{self.experiment_timestamp}.npy', preds)
        np.save(folder_path + f'true_{self.experiment_timestamp}.npy', trues)

        # ä¿®æ”¹ï¼šä¿å­˜åˆ°å¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬æ–‡ä»¶
        result_file = f"result_dual_branch_forecast_{self.experiment_timestamp}.txt"
        f = open(result_file, 'a', encoding='utf-8')
        f.write(f"Experiment Time: {self.detailed_timestamp}\n")
        f.write(f"Dataset: {self.args.data_path}\n")
        f.write(f"Model: {self.args.model}\n")
        f.write(timestamped_setting + "\n")
        f.write(f'mse:{mse:.6f}, mae:{mae:.6f}, rmse:{rmse:.6f}, mape:{mape:.6f}, mspe:{mspe:.6f}, R2:{r2:.6f}\n')
        f.write('\n')
        f.close()

        # åŒæ—¶ä¿å­˜åˆ°æ€»çš„ç»“æœæ–‡ä»¶ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        f = open("result_dual_branch_forecast.txt", 'a', encoding='utf-8')
        f.write(f"[{self.detailed_timestamp}] " + timestamped_setting + "\n")
        f.write(f'mse:{mse:.6f}, mae:{mae:.6f}, rmse:{rmse:.6f}, mape:{mape:.6f}, mspe:{mspe:.6f}, R2:{r2:.6f}\n')
        f.write('\n')
        f.close()

        return