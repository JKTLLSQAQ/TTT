import torch
import torch.nn as nn
import torch.nn.functional as F
from layers.Autoformer_EncDec import series_decomp
from layers.Embed import DataEmbedding_wo_pos
from layers.StandardNorm import Normalize
from layers.ChebyKANLayer import ChebyKANLinear
from layers.TimeDART_EncDec import Diffusion
from utils.RevIN import RevIN


class ChannelSTAR(nn.Module):
    """Channel-level STARæ¨¡å— - åŸºäºSOFTSè®ºæ–‡è®¾è®¡"""

    def __init__(self, d_model, d_core=None):
        super().__init__()
        self.d_model = d_model
        self.d_core = d_core if d_core is not None else d_model // 2

        # æ ¸å¿ƒè¡¨ç¤ºç”Ÿæˆç½‘ç»œ - å‚è€ƒSOFTSçš„MLPè®¾è®¡
        self.core_gen = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Linear(d_model, self.d_core)
        )

        # èåˆç½‘ç»œ - å‚è€ƒSOFTSçš„fusionè®¾è®¡
        self.fusion_net = nn.Sequential(
            nn.Linear(d_model + self.d_core, d_model),
            nn.GELU(),
            nn.Dropout(0.1)
        )

    def stochastic_pooling(self, x):
        """
        SOFTSé£æ ¼çš„éšæœºæ± åŒ– - åœ¨channelç»´åº¦èšåˆ
        x: [B, N, d_core] - Næ˜¯channelæ•°
        """
        batch_size, n_channels, core_dim = x.shape

        if self.training:
            # è®­ç»ƒæ—¶ï¼šæŒ‰æ¦‚ç‡éšæœºé‡‡æ ·channel
            probs = F.softmax(x, dim=1)  # [B, N, d_core] - åœ¨channelç»´åº¦è®¡ç®—æ¦‚ç‡

            # é‡æ–°å¡‘å½¢ä¾¿äºé‡‡æ ·ï¼š[B*d_core, N]
            probs_reshaped = probs.permute(0, 2, 1).contiguous().view(-1, n_channels)  # [B*d_core, N]
            x_reshaped = x.permute(0, 2, 1).contiguous().view(-1, n_channels)  # [B*d_core, N]

            # ä¸ºæ¯ä¸ª(batch, feature)å¯¹é‡‡æ ·ä¸€ä¸ªchannel
            sampled_indices = torch.multinomial(probs_reshaped, 1).squeeze(-1)  # [B*d_core]

            # æ”¶é›†é‡‡æ ·ç»“æœ
            batch_indices = torch.arange(batch_size * core_dim, device=x.device)
            sampled_values = x_reshaped[batch_indices, sampled_indices]  # [B*d_core]

            # é‡æ–°å¡‘å½¢ä¸ºæ ¸å¿ƒè¡¨ç¤ºï¼š[B, 1, d_core]
            core = sampled_values.view(batch_size, core_dim).unsqueeze(1)  # [B, 1, d_core]
        else:
            # æµ‹è¯•æ—¶ï¼šåŠ æƒå¹³å‡
            weights = F.softmax(x, dim=1)  # [B, N, d_core]
            core = torch.sum(x * weights, dim=1, keepdim=True)  # [B, 1, d_core]

        return core

    def forward(self, x):
        """
        x: [B, N, d_model] - Næ˜¯channelæ•°ï¼Œd_modelæ˜¯series embeddingç»´åº¦
        è¾“å‡º: [B, N, d_model] - å¢å¼ºåçš„channelç‰¹å¾
        """
        B, N, D = x.shape

        # ç”Ÿæˆæ¯ä¸ªchannelçš„æ ¸å¿ƒè¡¨ç¤ºå€™é€‰
        core_candidates = self.core_gen(x)  # [B, N, d_core]

        # éšæœºæ± åŒ–ç”Ÿæˆå…¨å±€æ ¸å¿ƒï¼ˆåœ¨channelç»´åº¦èšåˆï¼‰
        global_core = self.stochastic_pooling(core_candidates)  # [B, 1, d_core]

        # å°†å…¨å±€æ ¸å¿ƒåˆ†å‘åˆ°æ¯ä¸ªchannel
        global_core_expanded = global_core.expand(B, N, self.d_core)  # [B, N, d_core]

        # èåˆåŸå§‹channelç‰¹å¾å’Œå…¨å±€æ ¸å¿ƒ
        fused_input = torch.cat([x, global_core_expanded], dim=-1)  # [B, N, d_model + d_core]
        fused_output = self.fusion_net(fused_input)  # [B, N, d_model]

        # æ®‹å·®è¿æ¥
        return x + fused_output


class LightweightDiffusion(nn.Module):
    """è½»é‡çº§æ‰©æ•£æ¨¡å—"""

    def __init__(self, time_steps=20, device='cuda', scheduler='linear'):
        super().__init__()
        self.diffusion = Diffusion(time_steps=time_steps, device=device, scheduler=scheduler)

    def forward(self, x, apply_noise=True):
        if apply_noise and self.training:
            return self.diffusion(x)
        else:
            return x, None, None


class AdaptiveKANMixer(nn.Module):
    """è‡ªé€‚åº”KANæ··åˆå™¨"""

    def __init__(self, d_model, component_type='trend'):
        super().__init__()
        # æ ¹æ®åˆ†é‡ç±»å‹é€‰æ‹©KANé˜¶æ•°
        order_map = {'trend': 3, 'seasonal': 5, 'residual': 4}
        order = order_map.get(component_type, 4)

        self.kan_layer = ChebyKANLinear(d_model, d_model, order)
        self.conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, x):
        B, T, C = x.shape
        x_kan = self.kan_layer(x.reshape(B * T, C)).reshape(B, T, C)
        x_conv = self.conv(x.transpose(1, 2)).transpose(1, 2)
        return self.norm(x + x_kan + x_conv)


class ComponentProcessor(nn.Module):
    """åˆ†é‡å¤„ç†å™¨ - seasonalåˆ†é‡ä½¿ç”¨Channel-level STAR"""

    def __init__(self, configs, component_type):
        super().__init__()
        self.component_type = component_type

        if component_type == 'trend':
            self.processor = nn.Sequential(
                AdaptiveKANMixer(configs.d_model, 'trend'),
                nn.Linear(configs.d_model, configs.d_model),
                nn.GELU(),
                nn.Dropout(configs.dropout)
            )
        elif component_type == 'seasonal':
            # ğŸ”¥ ä½¿ç”¨Channel-level STAR + Diffusion + KAN
            self.channel_star = ChannelSTAR(configs.d_model, configs.d_model // 2)
            self.diffusion = LightweightDiffusion(time_steps=20, device=configs.device)
            self.processor = AdaptiveKANMixer(configs.d_model, 'seasonal')
        else:  # residual
            self.processor = nn.Sequential(
                nn.Linear(configs.d_model, configs.d_ff),
                nn.GELU(),
                nn.Linear(configs.d_ff, configs.d_model),
                nn.Dropout(configs.dropout)
            )

    def forward(self, x):
        if self.component_type == 'seasonal':
            # ğŸ”¥ æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¤„ç†ç»´åº¦
            B, T, D = x.shape

            # è½¬æ¢ä¸ºchannelç»´åº¦å¤„ç†ï¼šå‡è®¾æˆ‘ä»¬æœ‰å¤šä¸ªchannel
            # å¦‚æœæ˜¯å•å˜é‡ï¼Œæˆ‘ä»¬éœ€è¦åˆ›é€ channelç»´åº¦
            if self.training:
                # Channel-level STARå¤„ç†
                # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å…·ä½“æ•°æ®æ ¼å¼è°ƒæ•´
                # å‡è®¾xæ˜¯[B*N, T, d_model]æ ¼å¼ï¼ˆé€šé“ç‹¬ç«‹å¤„ç†åï¼‰

                # åº”ç”¨Channel STARï¼ˆéœ€è¦é‡æ–°ç»„ç»‡æ•°æ®ï¼‰
                # æš‚æ—¶è·³è¿‡Channel STARï¼Œå› ä¸ºåœ¨åˆ†é‡çº§åˆ«channelå·²ç»åˆ†ç¦»
                x_processed = x

                # åº”ç”¨æ‰©æ•£
                x_noise, noise, t = self.diffusion(x_processed, apply_noise=True)
                return self.processor(x_noise)
            else:
                x_processed = x
                return self.processor(x_processed)
        else:
            return self.processor(x)

class Model(nn.Module):
    """Channel-level STARæ¨¡å‹ - åŸºäºSOFTSè®¾è®¡æ€æƒ³"""

    def __init__(self, configs):
        super().__init__()
        self.configs = configs
        self.task_name = configs.task_name
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len

        # åˆ†è§£æ¨¡å—
        self.decomposition = series_decomp(configs.moving_avg)

        # ğŸ”¥ SOFTSé£æ ¼çš„Series Embedding - æ¯ä¸ªchannelç‹¬ç«‹embedding
        self.series_embedding = nn.Linear(configs.seq_len, configs.d_model)

        # ğŸ”¥ Channel-level STARæ¨¡å— - å¤šå±‚å †å 
        self.channel_star_layers = nn.ModuleList([
            ChannelSTAR(configs.d_model, configs.d_model // 2)
            for _ in range(getattr(configs, 'star_layers', 2))
        ])

        # åˆ†é‡å¤„ç†å™¨
        self.trend_processor = ComponentProcessor(configs, 'trend')
        self.seasonal_processor = ComponentProcessor(configs, 'seasonal')
        self.residual_processor = ComponentProcessor(configs, 'residual')

        # å½’ä¸€åŒ–
        self.revin_layer = RevIN(configs.enc_in, affine=True)

        # ğŸ”¥ SOFTSé£æ ¼çš„é¢„æµ‹å±‚ - ç›´æ¥ä»series representationé¢„æµ‹
        self.trend_predictor = nn.Linear(configs.d_model, configs.pred_len)
        self.seasonal_predictor = nn.Linear(configs.d_model, configs.pred_len)
        self.residual_predictor = nn.Linear(configs.d_model, configs.pred_len)

        # å¯å­¦ä¹ èåˆæƒé‡
        self.fusion_weights = nn.Parameter(torch.tensor([0.25, 0.5, 0.25]))  # [trend, seasonal, residual]

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):
        if self.task_name == 'long_term_forecast':
            return self.forecast(x_enc, x_mark_enc)
        else:
            raise ValueError('Only long_term_forecast implemented')

    def forecast(self, x_enc, x_mark_enc=None):
        B, T, N = x_enc.size()

        # å½’ä¸€åŒ–
        x_enc = self.revin_layer(x_enc, 'norm')

        # åˆ†è§£
        seasonal, trend = self.decomposition(x_enc)
        residual = x_enc - seasonal - trend

        # ğŸ”¥ SOFTSé£æ ¼çš„Series Embedding - æ¯ä¸ªchannelçš„æ—¶åºæ˜ å°„åˆ°embeddingç©ºé—´
        # [B, T, N] -> [B, N, T] -> [B, N, d_model]
        seasonal_series = self.series_embedding(seasonal.transpose(1, 2))  # [B, N, d_model]
        trend_series = self.series_embedding(trend.transpose(1, 2))  # [B, N, d_model]
        residual_series = self.series_embedding(residual.transpose(1, 2))  # [B, N, d_model]

        # ğŸ”¥ Channel-level STARå¤„ç† - åœ¨channelç»´åº¦äº¤äº’
        # å¯¹æ¯ä¸ªåˆ†é‡åˆ†åˆ«åº”ç”¨Channel STAR
        for star_layer in self.channel_star_layers:
            seasonal_series = star_layer(seasonal_series)
            trend_series = star_layer(trend_series)
            residual_series = star_layer(residual_series)

        # ç›´æ¥ä»series representationé¢„æµ‹ï¼ˆSOFTSé£æ ¼ï¼‰
        seasonal_pred = self.seasonal_predictor(seasonal_series)  # [B, N, pred_len]
        trend_pred = self.trend_predictor(trend_series)  # [B, N, pred_len]
        residual_pred = self.residual_predictor(residual_series)  # [B, N, pred_len]

        # åŠ æƒèåˆ
        weights = F.softmax(self.fusion_weights, dim=0)
        final_pred = (weights[0] * trend_pred +
                      weights[1] * seasonal_pred +
                      weights[2] * residual_pred)  # [B, N, pred_len]

        # è½¬æ¢å›æ—¶åºæ ¼å¼ï¼š[B, N, pred_len] -> [B, pred_len, N]
        final_pred = final_pred.transpose(1, 2)

        # åå½’ä¸€åŒ–
        final_pred = self.revin_layer(final_pred, 'denorm')
        return final_pred